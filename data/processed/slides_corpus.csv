doc_id,page_number,text
IR_ch1,1,introduction to information retrieval introducing information retrieval and web search
IR_ch1,2,information retrieval information retrieval ir is finding material usually documents of an unstructured nature usually text that satisfies an information need from within large collections usually stored on computers these days we frequently think first of web search but there are many other cases e mail search searching your laptop corporate knowledge bases legal information retrieval
IR_ch1,3,unstructured text vs structured database data in the mid nineties
IR_ch1,4,unstructured text vs structured database data today
IR_ch1,5,sec basic assumptions of information retrieval collection a set of documents assume it is a static collection for the moment goal retrieve documents with information that is relevant to the user s information need and helps the user complete a task
IR_ch1,6,the classic search model get rid of mice in a user task politically correct way misconception info about removing mice info need without killing them misformulation query searc how trap mice alive h search engine query results collection refinement
IR_ch1,7,sec how good are the retrieved docs precision fraction of retrieved docs that are relevant to the user s information need recall fraction of relevant docs in collection that are retrieved more precise definitions and measurements to follow later
IR_ch1,8,introduction to information retrieval term document incidence matrices
IR_ch1,9,sec unstructured data in which plays of shakespeare contain the words brutus and caesar but not calpurnia one could grep all of shakespeare s plays for brutus and caesar then strip out lines containing calpurnia why is that not the answer slow for large corpora not calpurnia is non trivial other operations e g find the word romans near countrymen not feasible ranked retrieval best documents to return later lectures
IR_ch1,10,sec term document incidence matrices antony and cleopatra julius caesar the tempest hamlet othello macbeth antony brutus caesar calpurnia cleopatra mercy worser if play contains brutus and caesar but not word otherwise calpurnia
IR_ch1,11,sec incidence vectors so we have a vector for each term to answer query take the vectors for brutus caesar and calpurnia complemented bitwise and and and antony and cleopatra julius caesar the tempest hamlet othello macbeth antony brutus caesar calpurnia cleopatra mercy worser
IR_ch1,12,sec answers to query antony and cleopatra act iii scene ii agrippa aside to domitius enobarbus why enobarbus when antony found julius caesar dead he cried almost to roaring and he wept when at philippi he found brutus slain hamlet act iii scene ii lord polonius i did enact julius caesar i was killed i the capitol brutus killed me
IR_ch1,13,sec bigger collections consider n million documents each with about words avg bytes word including spaces punctuation gb of data in the documents say there are m k distinct terms among these
IR_ch1,14,sec can t build the matrix k x m matrix has half a trillion s and s but it has no more than one billion s why matrix is extremely sparse what s a better representation we only record the positions
IR_ch1,15,introduction to information retrieval the inverted index the key data structure underlying modern ir
IR_ch1,16,sec inverted index for each term t we must store a list of all documents that contain t identify each doc by a docid a document serial number can we used fixed size arrays for this brutus caesar calpurnia what happens if the word caesar is added to document
IR_ch1,17,sec inverted index we need variable size postings lists on disk a continuous run of postings is normal and best posting in memory can use linked lists or variable length arrays brutus some tradeoffs in size ease of insertion caesar calpurnia postings dictionary sorted by docid more later on why
IR_ch1,18,sec inverted index construction documents to friends romans countrymen be indexed tokenizer token stream friends romans countrymen linguistic modules friend roman countryman modified tokens indexer friend roman inverted index countryman
IR_ch1,19,initial stages of text processing tokenization cut character sequence into word tokens deal with john s a state of the art solution normalization map text and query term to same form you want u s a and usa to match stemming we may wish different forms of a root to match authorize authorization stop words we may omit very common words or not the a to of
IR_ch1,20,sec indexer steps token sequence sequence of modified token document id pairs doc doc i did enact julius so let it be with caesar i was killed caesar the noble i the capitol brutus hath told you brutus killed me caesar was ambitious
IR_ch1,21,sec indexer steps sort sort by terms and then docid core indexing step
IR_ch1,22,sec indexer steps dictionary postings multiple term entries in a single document are merged split into dictionary and postings doc frequency information is added why frequency will discuss later
IR_ch1,23,sec where do we pay in storage lists of docids terms and counts ir system implementation how do we index efficiently how much storage do we need pointers
IR_ch1,24,introduction to information retrieval query processing with an inverted index
IR_ch1,25,sec the index we just built how do we process a query our focus later what kinds of queries can we process
IR_ch1,26,sec query processing and consider processing the query brutus and caesar locate brutus in the dictionary retrieve its postings locate caesar in the dictionary retrieve its postings merge the two postings intersect the document sets brutus caesar
IR_ch1,27,sec the merge walk through the two postings simultaneously in time linear in the total number of postings entries brutus caesar if the list lengths are x and y the merge takes o x y operations crucial postings sorted by docid
IR_ch1,28,intersecting two postings lists a merge algorithm
IR_ch1,29,introduction to information retrieval the boolean retrieval model extended boolean models
IR_ch1,30,sec boolean queries exact match the boolean retrieval model is being able to ask a query that is a boolean expression boolean queries are queries using and or and not to join query terms views each document as a set of words is precise document matches condition or not perhaps the simplest model to build an ir system on primary commercial retrieval tool for decades many search systems you still use are boolean email library catalog mac os x spotlight
IR_ch1,31,sec example westlaw http www westlaw com largest commercial paying subscribers legal search service started ranking added new federated search added tens of terabytes of data users majority of users still use boolean queries example query what is the statute of limitations in cases involving the federal tort claims act limit statute action s federal tort claim within words s in same sentence
IR_ch1,32,sec example westlaw http www westlaw com another example query requirements for disabled people to be able to access a workplace disabl p access s work site work place employment place note that space is disjunction not conjunction long precise queries proximity operators incrementally developed not like web search many professional searchers still like boolean search you know exactly what you are getting but that doesn t mean it actually works better
IR_ch1,33,sec boolean queries more general merges exercise adapt the merge for the queries brutus and not caesar brutus or not caesar can we still run through the merge in time o x y what can we achieve
IR_ch1,34,sec merging what about an arbitrary boolean formula brutus or caesar and not antony or cleopatra can we always merge in linear time linear in what can we do better
IR_ch1,35,sec query optimization what is the best order for query processing consider a query that is an and of n terms for each of the n terms get its postings brututshen and them tog ethe r caesar calpurnia query brutus and calpurnia and caesar
IR_ch1,36,sec query optimization example process in order of increasing freq start with smallest set then keep cutting further this is why we kept document freq in dictionary brutus caesar calpurnia execute the query as calpurnia and brutus and caesar
IR_ch1,37,sec more general optimization e g madding or crowd and ignoble or strife get doc freq s for all terms estimate the size of each or by the sum of its doc freq s conservative process in increasing order of or sizes
IR_ch1,38,exercise recommend a query processing order for which two terms should we process first t e k m s t t e a r y a k r a n e e l i m s e r e g e i m s s d e o a r i s l a n c d e o e p e f r e q tangerine or trees and marmalade or skies and kaleidoscope or eyes
IR_ch1,39,query processing exercises exercise if the query is friends and romans and not countrymen how could we use the freq of countrymen exercise extend the merge to an arbitrary boolean query can we always guarantee execution in time linear in the total postings size hint begin with the case of a boolean formula query in this each query term appears only once in the query
IR_ch1,40,exercise try the search feature at http www rhymezone com shakespeare write down five search features you think it could do better
IR_ch1,41,introduction to information retrieval phrase queries and positional indexes
IR_ch1,42,sec phrase queries we want to be able to answer queries such as stanford university as a phrase thus the sentence i went to university at stanford is not a match the concept of phrase queries has proven easily understood by users one of the few advanced search ideas that works many more queries are implicit phrase queries for this it no longer suffices to store only term docs entries
IR_ch1,43,sec a first attempt biword indexes index every consecutive pair of terms in the text as a phrase for example the text friends romans countrymen would generate the biwords friends romans romans countrymen each of these biwords is now a dictionary term two word phrase query processing is now immediate
IR_ch1,44,sec longer phrase queries longer phrases can be processed by breaking them down stanford university palo alto can be broken into the boolean query on biwords stanford university and university palo and palo alto without the docs we cannot verify that the docs matching the above boolean query do contain the phrase can have false positives
IR_ch1,45,sec issues for biword indexes false positives as noted before index blowup due to bigger dictionary infeasible for more than biwords big even for them biword indexes are not the standard solution for all biwords but can be part of a compound strategy
IR_ch1,46,sec solution positional indexes in the postings store for each term the position s in which tokens of it appear term number of docs containing term doc position position doc position position etc
IR_ch1,47,sec positional index example be which of docs could contain to be or not to be for phrase queries we use a merge algorithm recursively at the document level but we now need to deal with more than just equality
IR_ch1,48,sec processing a phrase query extract inverted index entries for each distinct term to be or not merge their doc position lists to enumerate all positions with to be or not to be to be same general method for proximity searches
IR_ch1,49,sec proximity queries limit statute federal tort again here k means within k words of clearly positional indexes can be used for such queries biword indexes cannot exercise adapt the linear merge of postings to handle proximity queries can you make it work for any value of k this is a little tricky to do correctly and efficiently see figure of iir
IR_ch1,50,sec positional index size a positional index expands postings storage substantially even though indices can be compressed nevertheless a positional index is now standardly used because of the power and usefulness of phrase and proximity queries whether used explicitly or implicitly in a ranking retrieval system
IR_ch1,51,sec positional index size need an entry for each occurrence not just once per document index size depends on average document sizewhy average web page has terms sec filings books even some epic poems easily terms consider a term with frequency document size postings positional postings
IR_ch1,52,sec rules of thumb a positional index is as large as a non positional index positional index size of volume of original text caveat all of this holds for english like languages
IR_ch1,53,sec combination schemes these two approaches can be profitably combined for particular phrases michael jackson britney spears it is inefficient to keep on merging positional postings lists even more so for phrases like the who williams et al evaluate a more sophisticated mixed indexing scheme a typical web query mixture was executed in ¼ of the time of using just a positional index it required more space than having a positional index alone
IR_ch1,54,introduction to information retrieval structured vs unstructured data
IR_ch1,55,ir vs databases structured vs unstructured data structured data tends to refer to information in tables employee manager salary smith jones chang smith ivy smith typically allows numerical range and exact match for text queries e g salary and manager smith
IR_ch1,56,unstructured data typically refers to free text allows keyword queries including operators more sophisticated concept queries e g find all web pages dealing with drug abuse classic model for searching text documents
IR_ch1,57,semi structured data in fact almost no data is unstructured e g this slide has distinctly identified zones such as the title and bullets to say nothing of linguistic structure facilitates semi structured search such as title contains data and bullets contain search or even title is about object oriented programming and author something like stro rup where is the wild card operator
IR_ch2,1,introduction to information retrieval introduction to information retrieval introducing information retrieval and web search
IR_ch2,2,introduction to information retrieval information retrieval information retrieval ir is finding material usually documents of an unstructured nature usually text that satisfies an information need from within large collections usually stored on computers these days we frequently think first of web search but there are many other cases e mail search searching your laptop corporate knowledge bases legal information retrieval
IR_ch2,3,introduction to information retrieval unstructured text vs structured database data in the mid nineties unstructured structured data volume market cap
IR_ch2,4,introduction to information retrieval unstructured text vs structured database data today unstructured structured data volume market cap
IR_ch2,5,introduction to information retrieval sec basic assumptions of information retrieval collection a set of documents assume it is a static collection for the moment goal retrieve documents with information that is relevant to the user s information need and helps the user complete a task
IR_ch2,6,introduction to information retrieval the classic search model get rid of mice in a user task politically correct way misconception info about removing mice info need without killing them misformulation query how trap mice alive search search engine query results collection refinement
IR_ch2,7,introduction to information retrieval sec how good are the retrieved docs precision fraction of retrieved docs that are relevant to the user s information need recall fraction of relevant docs in collection that are retrieved more precise definitions and measurements to follow later
IR_ch2,8,introduction to information retrieval introduction to information retrieval term document incidence matrices
IR_ch2,9,introduction to information retrieval sec unstructured data in which plays of shakespeare contain the words brutus and caesar but not calpurnia one could grep all of shakespeare s plays for brutus and caesar then strip out lines containing calpurnia why is that not the answer slow for large corpora not calpurnia is non trivial other operations e g find the word romans near countrymen not feasible ranked retrieval best documents to return later lectures
IR_ch2,10,introduction to information retrieval sec term document incidence matrices antony and cleopatra julius caesar the tempest hamlet othello macbeth antony brutus caesar calpurnia cleopatra mercy worser if play contains brutus and caesar but not word otherwise calpurnia
IR_ch2,11,introduction to information retrieval sec incidence vectors so we have a vector for each term to answer query take the vectors for brutus caesar and calpurnia complemented bitwise and and and antony and cleopatra julius caesar the tempest hamlet othello macbeth antony brutus caesar calpurnia cleopatra mercy worser
IR_ch2,12,introduction to information retrieval sec answers to query antony and cleopatra act iii scene ii agrippa aside to domitius enobarbus why enobarbus when antony found julius caesar dead he cried almost to roaring and he wept when at philippi he found brutus slain hamlet act iii scene ii lord polonius i did enact julius caesar i was killed i the capitol brutus killed me
IR_ch2,13,introduction to information retrieval sec bigger collections consider n million documents each with about words avg bytes word including spaces punctuation gb of data in the documents say there are m k distinct terms among these
IR_ch2,14,introduction to information retrieval sec can t build the matrix k x m matrix has half a trillion s and s but it has no more than one billion s why matrix is extremely sparse what s a better representation we only record the positions
IR_ch2,15,introduction to information retrieval introduction to information retrieval the inverted index the key data structure underlying modern ir
IR_ch2,16,introduction to information retrieval sec inverted index for each term t we must store a list of all documents that contain t identify each doc by a docid a document serial number can we used fixed size arrays for this brutus caesar calpurnia what happens if the word caesar is added to document
IR_ch2,17,introduction to information retrieval sec inverted index we need variable size postings lists on disk a continuous run of postings is normal and best in memory can use linked lists or variable length arrays some tradeoffs in size ease of insertion posting brutus caesar calpurnia postings dictionary sorted by docid more later on why
IR_ch2,18,introduction to information retrieval sec inverted index construction documents to friends romans countrymen be indexed tokenizer token stream friends romans countrymen linguistic modules friend roman countryman modified tokens indexer friend roman inverted index countryman
IR_ch2,19,introduction to information retrieval initial stages of text processing tokenization cut character sequence into word tokens deal with john s a state of the art solution normalization map text and query term to same form you want u s a and usa to match stemming we may wish different forms of a root to match authorize authorization stop words we may omit very common words or not the a to of
IR_ch2,20,introduction to information retrieval sec indexer steps token sequence sequence of modified token document id pairs doc doc i did enact julius so let it be with caesar i was killed caesar the noble i the capitol brutus hath told you brutus killed me caesar was ambitious
IR_ch2,21,introduction to information retrieval sec indexer steps sort sort by terms at least conceptually and then docid core indexing step
IR_ch2,22,introduction to information retrieval sec indexer steps dictionary postings multiple term entries in a single document are merged split into dictionary and postings doc frequency information is added why frequency will discuss later
IR_ch2,23,introduction to information retrieval sec where do we pay in storage lists of docids terms and counts ir system implementation how do we index efficiently how much storage do we need pointers
IR_ch2,24,introduction to information retrieval introduction to information retrieval query processing with an inverted index
IR_ch2,25,introduction to information retrieval sec the index we just built how do we process a query our focus later what kinds of queries can we process
IR_ch2,26,introduction to information retrieval sec query processing and consider processing the query brutus and caesar locate brutus in the dictionary retrieve its postings locate caesar in the dictionary retrieve its postings merge the two postings intersect the document sets brutus caesar
IR_ch2,27,introduction to information retrieval sec the merge walk through the two postings simultaneously in time linear in the total number of postings entries brutus caesar if the list lengths are x and y the merge takes o x y operations crucial postings sorted by docid
IR_ch2,28,introduction to information retrieval intersecting two postings lists a merge algorithm
IR_ch2,29,introduction to information retrieval introduction to information retrieval the boolean retrieval model extended boolean models
IR_ch2,30,introduction to information retrieval sec boolean queries exact match the boolean retrieval model is being able to ask a query that is a boolean expression boolean queries are queries using and or and not to join query terms views each document as a set of words is precise document matches condition or not perhaps the simplest model to build an ir system on primary commercial retrieval tool for decades many search systems you still use are boolean email library catalog macos spotlight
IR_ch2,31,introduction to information retrieval sec example westlaw http www westlaw com largest commercial paying subscribers legal search service started ranking added new federated search added tens of terabytes of data users majority of users still use boolean queries example query what is the statute of limitations in cases involving the federal tort claims act limit statute action s federal tort claim within words s in same sentence
IR_ch2,32,introduction to information retrieval sec example westlaw http www westlaw com another example query requirements for disabled people to be able to access a workplace disabl p access s work site work place employment place note that space is disjunction not conjunction long precise queries proximity operators incrementally developed not like web search many professional searchers still like boolean search you know exactly what you are getting but that doesn t mean it actually works better
IR_ch2,33,introduction to information retrieval sec boolean queries more general merges exercise adapt the merge for the queries brutus and not caesar brutus or not caesar can we still run through the merge in time o x y what can we achieve
IR_ch2,34,introduction to information retrieval sec merging what about an arbitrary boolean formula brutus or caesar and not antony or cleopatra can we always merge in linear time linear in what can we do better
IR_ch2,35,introduction to information retrieval sec query optimization what is the best order for query processing consider a query that is an and of n terms for each of the n terms get its postings then and them together brutus caesar calpurnia query brutus and calpurnia and caesar
IR_ch2,36,introduction to information retrieval sec query optimization example process in order of increasing freq start with smallest set then keep cutting further this is why we kept document freq in dictionary brutus caesar calpurnia execute the query as calpurnia and brutus and caesar
IR_ch2,37,introduction to information retrieval exercise recommend a query processing order for which two terms should we process first t e k m s t t e a r y a k r a n e e l i m s e r e g e i m s s d e o a r i s l a n c d e o e p e f r e q tangerine or trees and marmalade or skies and kaleidoscope or eyes
IR_ch2,38,introduction to information retrieval sec more general optimization e g madding or crowd and ignoble or strife get doc freq s for all terms estimate the size of each or by the sum of its doc freq s conservative process in increasing order of or sizes
IR_ch2,39,introduction to information retrieval query processing exercises exercise if the query is friends and romans and not countrymen how could we use the freq of countrymen exercise extend the merge to an arbitrary boolean query can we always guarantee execution in time linear in the total postings size hint begin with the case of a boolean formula query in this each query term appears only once in the query
IR_ch2,40,introduction to information retrieval exercise try the search feature at http www rhymezone com shakespeare write down five search features you think it could do better
IR_ch2,41,introduction to information retrieval introduction to information retrieval phrase queries and positional indexes
IR_ch2,42,introduction to information retrieval sec phrase queries we want to be able to answer queries such as stanford university as a phrase thus the sentence i went to university at stanford is not a match the concept of phrase queries has proven easily understood by users one of the few advanced search ideas that works many more queries are implicit phrase queries for this it no longer suffices to store only term docs entries
IR_ch2,43,introduction to information retrieval sec a first attempt biword indexes index every consecutive pair of terms in the text as a phrase for example the text friends romans countrymen would generate the biwords friends romans romans countrymen each of these biwords is now a dictionary term two word phrase query processing is now immediate
IR_ch2,44,introduction to information retrieval sec longer phrase queries longer phrases can be processed by breaking them down stanford university palo alto can be broken into the boolean query on biwords stanford university and university palo and palo alto without the docs we cannot verify that the docs matching the above boolean query do contain the phrase can have false positives
IR_ch2,45,introduction to information retrieval sec issues for biword indexes false positives as noted before index blowup due to bigger dictionary infeasible for more than biwords big even for them biword indexes are not the standard solution for all biwords but can be part of a compound strategy
IR_ch2,46,introduction to information retrieval sec solution positional indexes in the postings store for each term the position s in which tokens of it appear term number of docs containing term doc position position doc position position etc
IR_ch2,47,introduction to information retrieval sec positional index example be which of docs could contain to be or not to be for phrase queries we use a merge algorithm recursively at the document level but we now need to deal with more than just equality
IR_ch2,48,introduction to information retrieval sec processing a phrase query extract inverted index entries for each distinct term to be or not merge their doc position lists to enumerate all positions with to be or not to be to be same general method for proximity searches
IR_ch2,49,introduction to information retrieval sec proximity queries limit statute federal tort again here k means within k words of clearly positional indexes can be used for such queries biword indexes cannot exercise adapt the linear merge of postings to handle proximity queries can you make it work for any value of k this is a little tricky to do correctly and efficiently see figure of iir
IR_ch2,50,introduction to information retrieval sec positional index size a positional index expands postings storage substantially even though indices can be compressed nevertheless a positional index is now standardly used because of the power and usefulness of phrase and proximity queries whether used explicitly or implicitly in a ranking retrieval system
IR_ch2,51,introduction to information retrieval sec positional index size need an entry for each occurrence not just once per document index size depends on average document size why average web page has terms sec filings books even some epic poems easily terms consider a term with frequency document size postings positional postings
IR_ch2,52,introduction to information retrieval sec rules of thumb a positional index is as large as a non positional index positional index size of volume of original text caveat all of this holds for english like languages
IR_ch2,53,introduction to information retrieval sec combination schemes these two approaches can be profitably combined for particular phrases michael jackson britney spears it is inefficient to keep on merging positional postings lists even more so for phrases like the who williams et al evaluate a more sophisticated mixed indexing scheme a typical web query mixture was executed in ¼ of the time of using just a positional index it required more space than having a positional index alone
IR_ch3,1,introduction to information retrieval introduction to information retrieval cs information retrieval and web search basic inverted index construction
IR_ch3,2,introduction to information retrieval ch index construction how do we construct an index what strategies can we use with limited main memory
IR_ch3,3,introduction to information retrieval sec recall index construction term doc i did documents are parsed to extract words and these enact julius are saved with the document id caesar i was killed i the capitol brutus killed me doc doc so let it be i did enact julius with so let it be with caesar the caesar i was killed caesar the noble noble brutus i the capitol brutus hath told you hath told brutus killed me you caesar was ambitious caesar was ambitious
IR_ch3,4,introduction to information retrieval t e r m id id e n a c t ju liu s c a e s a r iw a s k ille d i t h e c a p it o l b r u t u s k ille d m e s o le t itb e w it h c a e s a r t h e n o b le b r u t u s h a t h t o ld y o u c a e s a r w a s a m b it io u s d o c t e r m a m b it io b e b r u t u s b r u t u s c a p it o l c a e s a r c a e s a r c a e s a r d id e n a c t h a t h ii i itju liu s k ille d k ille d le t m e n o b le s o t h e t h e t o ld y o u w a s w a s w it h u s d o c sec key step after all documents have been parsed the inverted file is sorted by terms we focus on this sort step
IR_ch3,5,introduction to information retrieval sec rcv our collection for this lecture as an example for applying scalable index construction algorithms we will use the reuters rcv collection this is one year of reuters newswire part of and the collection isn t really large enough but it s publicly available and is a plausible example
IR_ch3,6,introduction to information retrieval sec a reuters rcv document
IR_ch3,7,introduction to information retrieval sec reuters rcv statistics symbol statistic value n documents l avg tokens per doc m terms word types avg bytes per token incl spaces punct avg bytes per token without spaces punct avg bytes per term non positional postings bytes per word token vs bytes per word type why
IR_ch3,8,introduction to information retrieval sec sort based index construction as we build the index we parse docs one at a time the final postings for any term are incomplete until the end at bytes per termid docid demands a lot of space for large collections t in the case of rcv so we can do this in memory today but typical collections are much larger e g the new york times provides an index of years of newswire thus we need to store intermediate results on disk
IR_ch3,9,introduction to information retrieval sec scaling index construction in memory index construction does not scale can t stuff entire collection into memory sort then write back how can we construct an index for very large collections taking into account hardware constraints memory disk speed etc let s review some hardware basics
IR_ch3,10,introduction to information retrieval sec hardware basics servers used in ir systems now typically have several gb of main memory sometimes tens of gb available disk space is several orders of magnitude larger fault tolerance is very expensive it s much cheaper to use many regular machines rather than one fault tolerant machine
IR_ch3,11,introduction to information retrieval sec hardware basics access to data in memory is much faster than access to data on disk disk seeks no data is transferred from disk while the disk head is being positioned therefore transferring one large chunk of data from disk to memory is faster than transferring many small chunks disk i o is block based reading and writing of entire blocks as opposed to smaller chunks block sizes kb to kb
IR_ch3,12,introduction to information retrieval sec hardware assumptions circa symbol statistic value s average seek time ms x s b transfer time per byte μs x s processor s clock rate s p low level operation μs s e g compare swap a word size of main memory several gb size of disk space tb or more
IR_ch3,13,introduction to information retrieval sec sort using disk as memory can we use the same index construction algorithm for larger collections but by using disk instead of memory no sorting t records on disk is too slow too many disk seeks we need an external sorting algorithm
IR_ch3,14,introduction to information retrieval introduction to information retrieval cs information retrieval and web search external memory indexing
IR_ch3,15,introduction to information retrieval sec bsbi blocked sort based indexing sorting with fewer disk seeks byte records termid docid these are generated as we parse docs must now sort m such byte records by termid define a block m such records can easily fit a couple into memory will have such blocks to start with basic idea of algorithm accumulate postings for each block sort write to disk then merge the blocks into one long sorted order
IR_ch3,16,introduction to information retrieval sec
IR_ch3,17,introduction to information retrieval sec sorting blocks of m records first read each block and sort within quicksort takes o n ln n expected steps in our case n m times this estimate gives us sorted runs of m records each done straightforwardly need copies of data on disk but can optimize this
IR_ch3,18,introduction to information retrieval sec how to merge the sorted runs can do binary merges with a merge tree of log layers during each layer read into memory runs in blocks of m merge write back brutus d d d d brutus d d brutus d d caesar d d d d d caesar d d d caesar d d julius d noble d julius d killed d with d d d d killed d noble d with d d d d postings lists merged to be merged postings list disk
IR_ch3,19,introduction to information retrieval sec how to merge the sorted runs but it is more efficient to do a multi way merge where you are reading from all blocks simultaneously open all block files simultaneously and maintain a read buffer for each one and a write buffer for the output file in each iteration pick the lowest termid that hasn t been processed using a priority queue merge all postings lists for that termid and write it out providing you read decent sized chunks of each block into memory and then write out a decent sized output chunk then you re not killed by disk seeks
IR_ch3,20,introduction to information retrieval sec remaining problem with sort based algorithm our assumption was we can keep the dictionary in memory we need the dictionary which grows dynamically in order to implement a term to termid mapping
IR_ch3,21,introduction to information retrieval sec spimi single pass in memory indexing key idea generate separate dictionaries for each block no need to maintain term termid mapping across blocks key idea don t sort accumulate postings in postings lists as they occur with these two ideas we can generate a complete inverted index for each block these separate indexes can then be merged into one big index
IR_ch3,22,introduction to information retrieval sec spimi invert merging of blocks is analogous to bsbi
IR_ch3,23,introduction to information retrieval spimi in action sorted input token dictionary dictionary brutus d d caesar d brutus d d with d caesar d d d with d d d d brutus d noble d noble d caesar d with d d d d with d caesar d d d brutus d with d caesar d noble d with d
IR_ch3,24,introduction to information retrieval sec spimi compression compression makes spimi even more efficient compression of terms compression of postings more on this later original publication on spimi heinz and zobel
IR_ch3,25,introduction to information retrieval introduction to information retrieval cs information retrieval and web search distributed indexing
IR_ch3,26,introduction to information retrieval sec distributed indexing for web scale indexing don t try this at home must use a distributed computing cluster individual machines are fault prone can unpredictably slow down or fail how do we exploit such a pool of machines
IR_ch3,27,introduction to information retrieval sec web search engine data centers web search data centers google bing baidu mainly contain commodity machines data centers are distributed around the world estimate google million servers million processors cores gartner
IR_ch3,28,introduction to information retrieval sec massive data centers if in a non fault tolerant system with nodes each node has uptime what is the uptime of the entire system answer meaning of the time one or more servers is down exercise calculate the number of servers failing per minute for an installation of million servers
IR_ch3,29,introduction to information retrieval sec distributed indexing maintain a master machine directing the indexing job considered safe break up indexing into sets of parallel tasks master machine assigns each task to an idle machine from a pool
IR_ch3,30,introduction to information retrieval sec parallel tasks we will use two sets of parallel tasks parsers inverters break the input document collection into splits each split is a subset of documents corresponding to blocks in bsbi spimi
IR_ch3,31,introduction to information retrieval sec data flow master assign assign postings parser a f g p q z inverter a f parser a f g p q z inverter g p splits inverter q z parser a f g p q z map reduce segment files phase phase
IR_ch3,32,introduction to information retrieval sec parsers master assigns a split to an idle parser machine parser reads a document at a time and emits term doc pairs parser writes pairs into j partitions example each partition is for a range of terms first letters e g a f g p q z here j now to complete the index inversion
IR_ch3,33,introduction to information retrieval sec inverters an inverter collects all term doc pairs postings for one term partition sorts and writes to postings lists
IR_ch3,34,introduction to information retrieval example for index construction map caesar conquered d c came c c ed d c died c d came d c d c ed d c d died d reduce c d d d died d came d c ed d c d d died d came d c ed d
IR_ch3,35,introduction to information retrieval sec index construction index construction was just one phase another phase transforming a term partitioned index into a document partitioned index term partitioned one machine handles a subrange of terms document partitioned one machine handles a subrange of documents as we ll discuss in the web part of the course most search engines use a document partitioned index better load balancing etc
IR_ch3,36,introduction to information retrieval sec mapreduce the index construction algorithm we just described is an instance of mapreduce mapreduce dean and ghemawat is a robust and conceptually simple framework for distributed computing without having to write code for the distribution part they describe the google indexing system ca as consisting of a number of phases each implemented in mapreduce
IR_ch3,37,introduction to information retrieval sec schema for index construction in mapreduce schema of map and reduce functions map input list k v reduce k list v output instantiation of the schema for index construction map collection list termid docid reduce termid list docid termid list docid postings list postings list
IR_ch3,38,introduction to information retrieval introduction to information retrieval cs information retrieval and web search dynamic indexing
IR_ch3,39,introduction to information retrieval sec dynamic indexing up to now we have assumed that collections are static they rarely are documents come in over time and need to be inserted documents are deleted and modified this means that the dictionary and postings lists have to be modified postings updates for terms already in dictionary new terms added to dictionary
IR_ch3,40,introduction to information retrieval sec simplest approach maintain big main index new docs go into small auxiliary index search across both merge results deletions invalidation bit vector for deleted docs filter docs output on a search result by this invalidation bit vector periodically re index into one main index
IR_ch3,41,introduction to information retrieval sec issues with main and auxiliary indexes problem of frequent merges you touch stuff a lot poor performance during merge actually merging of the auxiliary index into the main index is efficient if we keep a separate file for each postings list merge is the same as a simple append but then we would need a lot of files inefficient for os assumption for the rest of the lecture the index is one big file in reality use a scheme somewhere in between e g split very large postings lists collect postings lists of length in one file etc
IR_ch3,42,introduction to information retrieval sec logarithmic merge maintain a series of indexes each twice as large as the previous one at any time some of these powers of are instantiated keep smallest z in memory larger ones i i on disk if z gets too big n write to disk as i or merge with i if i already exists as z either write merge z to disk as i if no i or merge with i to form z
IR_ch3,43,introduction to information retrieval logarithmic merge in action n z z z z n i i i i n i i i i n i n n
IR_ch3,44,introduction to information retrieval sec
IR_ch3,45,introduction to information retrieval sec logarithmic merge auxiliary and main index t n merges where t is of postings and n is size of auxiliary index construction time is o t n as in the worst case a posting is touched t n times logarithmic merge each posting is merged at most o log t n times so complexity is o t log t n so logarithmic merge is much more efficient for index construction but query processing now requires the merging of o log t n indexes whereas it is o if you just have a main and auxiliary index
IR_ch3,46,introduction to information retrieval sec further issues with multiple indexes collection wide statistics are hard to maintain e g when we speak of spell correction which of several corrected alternatives do we present to the user we may want to pick the one with the most hits how do we maintain the top ones with multiple indexes and invalidation bit vectors one possibility ignore everything but the main index for such ordering will see more such statistics used in results ranking
IR_ch3,47,introduction to information retrieval sec dynamic indexing at search engines all the large search engines now do dynamic indexing their indices have frequent incremental changes news items blogs new topical web pages but sometimes typically they also periodically reconstruct the index from scratch query processing is then switched to the new index and the old index is deleted
IR_ch3,48,introduction to information retrieval earlybird real time search at twitter requirements for real time search low latency high throughput query evaluation high ingestion rate and immediate data availability concurrent reads and writes of the index dominance of temporal signal
IR_ch3,49,introduction to information retrieval earlybird index organization earlybird consists of multiple index segments each segment is relatively small holding up to tweets each posting in a segment is a bit word bits for the tweet id and bits for the position in the tweet only one segment can be written to at any given time small enough to be in memory new postings are simply appended to the postings list but the postings list is traversed backwards to prioritize newer tweets the remaining segments are optimized for read only postings sorted in reverse chronological order newest first
IR_ch3,50,introduction to information retrieval sec other sorts of indexes positional indexes same sort of sorting problem just larger why building character n gram indexes as text is parsed enumerate n grams for each n gram need pointers to all dictionary terms containing it the postings
IR_ch3,51,introduction to information retrieval ch resources for today s lecture chapter of iir mg chapter original publication on mapreduce dean and ghemawat original publication on spimi heinz and zobel earlybird busch et al icde
IR_ch4,1,introduction to information retrieval introduction to information retrieval cs information retrieval and web search christopher manning and pandu nayak lecture index compression
IR_ch4,2,introduction to information retrieval last lecture index construction sort based indexing naïve in memory inversion blocked sort based indexing bsbi merge sort is effective for hard disk based sorting avoid seeks single pass in memory indexing spimi no global dictionary generate separate dictionary for each block don t sort postings accumulate postings in postings lists as they occur distributed indexing using mapreduce dynamic indexing multiple indices logarithmic merge
IR_ch4,3,introduction to information retrieval ch today collection statistics in more detail with rcv how big will the dictionary and postings be dictionary compression postings compression
IR_ch4,4,introduction to information retrieval ch why compression in general use less disk space save a little money give users more space keep more stuff in memory increases speed increase speed of data transfer from disk to memory read compressed data decompress is faster than read uncompressed data premise decompression algorithms are fast true of the decompression algorithms we use
IR_ch4,5,introduction to information retrieval ch why compression for inverted indexes dictionary make it small enough to keep in main memory make it so small that you can keep some postings lists in main memory too postings file s reduce disk space needed decrease time needed to read postings lists from disk large search engines keep a significant part of the postings in memory compression lets you keep more in memory we will devise various ir specific compression schemes
IR_ch4,6,introduction to information retrieval sec recall reuters rcv symbol statistic value n documents l avg tokens per doc m terms word types avg bytes per token incl spaces punct avg bytes per token without spaces punct avg bytes per term non positional postings
IR_ch4,7,introduction to information retrieval sec index parameters vs what we index details iir table p size of word types terms non positional positional postings postings dictionary non positional index positional index size cumul size k cumul size k cumul k unfiltered no numbers case folding stopwords stopwords stemming exercise give intuitions for all the entries why do some zero entries correspond to big deltas in other columns
IR_ch4,8,introduction to information retrieval sec lossless vs lossy compression lossless compression all information is preserved what we mostly do in ir lossy compression discard some information several of the preprocessing steps can be viewed as lossy compression case folding stop words stemming number elimination chapter prune postings entries that are unlikely to turn up in the top k list for any query almost no loss of quality in top k list
IR_ch4,9,introduction to information retrieval sec vocabulary size vs collection size how big is the term vocabulary that is how many distinct words are there can we assume an upper bound not really at least different words of length in practice the vocabulary will keep growing with the collection size especially with unicode
IR_ch4,10,introduction to information retrieval sec vocabulary size vs collection size heaps law m ktb m is the size of the vocabulary t is the number of tokens in the collection typical values k and b in a log log plot of vocabulary size m vs t heaps law predicts a line with slope about ½ it is the simplest possible linear relationship between the two in log log space log m log k b log t an empirical finding empirical law
IR_ch4,11,introduction to information retrieval sec heaps law fig p for rcv the dashed line log m log t is the best least squares fit thus m t so k and b good empirical fit for reuters rcv for first tokens law predicts terms actually terms
IR_ch4,12,introduction to information retrieval sec exercises what is the effect of including spelling errors vs automatically correcting spelling errors on heaps law compute the vocabulary size m for this scenario looking at a collection of web pages you find that there are different terms in the first tokens and different terms in the first tokens assume a search engine indexes a total of pages containing tokens on average what is the size of the vocabulary of the indexed collection as predicted by heaps law
IR_ch4,13,introduction to information retrieval sec zipf s law heaps law gives the vocabulary size in collections we also study the relative frequencies of terms in natural language there are a few very frequent terms and very many very rare terms zipf s law the ith most frequent term has frequency proportional to i cf i k i where k is a normalizing constant i cf is collection frequency the number of i occurrences of the term t in the collection i
IR_ch4,14,introduction to information retrieval sec zipf consequences if the most frequent term the occurs cf times then the second most frequent term of occurs cf times the third most frequent term and occurs cf times equivalent cf k i where k is a normalizing factor i so log cf log k log i i linear relationship between log cf and log i i another power law relationship
IR_ch4,15,introduction to information retrieval sec zipf s law for reuters rcv
IR_ch4,16,introduction to information retrieval ch compression now we will consider compressing the space for the dictionary and postings we ll do basic boolean index only no study of positional indexes etc but these ideas can be extended we will consider compression schemes
IR_ch4,17,introduction to information retrieval sec dictionary compression
IR_ch4,18,introduction to information retrieval sec why compress the dictionary search begins with the dictionary we want to keep it in memory memory footprint competition with other applications embedded mobile devices may have very little memory even if the dictionary isn t in memory we want it to be small for a fast search startup time so compressing the dictionary is important
IR_ch4,19,introduction to information retrieval dictionary storage naïve version array of fixed width entries terms bytes term mb t a a z a u e r c h l u m e s n f r e q p o s t i n g s p t r sec bytes bytes each dictionary search structure
IR_ch4,20,introduction to information retrieval sec fixed width terms are wasteful most of the bytes in the term column are wasted we allot bytes for letter terms and we still can t handle supercalifragilisticexpialidocious or hydrochlorofluorocarbons written english averages characters word exercise why is isn t this the number to use for estimating the dictionary size ave dictionary word in english characters how do we use characters per dictionary term short words dominate token counts but not type average
IR_ch4,21,introduction to information retrieval compressing the term list dictionary as a string systilesyzygeticsyzygialsyzygyszaibelyiteszczecinszomo f r e q p o s t i n g s p t r t e r m p t r sec store dictionary as a long string of characters pointer to next word shows end of current word hope to save up to of dictionary space total string length k x b mb pointers resolve m positions log m bits bytes
IR_ch4,22,introduction to information retrieval sec space for dictionary as a string bytes per term for freq now avg bytes per term for pointer to postings bytes term not bytes per term pointer avg bytes per term in term string k terms x mb against mb for fixed width
IR_ch4,23,introduction to information retrieval sec blocking store pointers to every kth term string example below k need to store term lengths extra byte systile syzygetic syzygial syzygy szaibelyite szczecin szomo freq postings ptr term ptr save bytes lose bytes on on term lengths pointers
IR_ch4,24,introduction to information retrieval sec blocking net gains example for block size k where we used bytes pointer without blocking x bytes now we use bytes shaved another mb this reduces the size of the dictionary from mb to mb we can save more with larger k question why not go with larger k
IR_ch4,25,introduction to information retrieval sec dictionary search without blocking assuming each dictionary term equally likely in query not really so in practice average number of comparisons exercise what if the frequencies of query terms were non uniform but known how would you structure the dictionary search tree
IR_ch4,26,introduction to information retrieval sec dictionary search with blocking binary search down to term block then linear search through terms in block blocks of binary tree avg compares
IR_ch4,27,introduction to information retrieval sec exercises estimate the space usage and savings compared to mb with blocking for block sizes of k and estimate the impact on search performance and slowdown compared to k with blocking for block sizes of k and
IR_ch4,28,introduction to information retrieval sec front coding front coding sorted words commonly have long common prefix store differences only for last k in a block of k automata automate automatic automation automat a e ic ion extra length encodes prefix automat beyond automat begins to resemble general string compression
IR_ch4,29,introduction to information retrieval sec rcv dictionary compression summary technique size in mb fixed width dictionary as string with pointers to every term blocking k blocking front coding
IR_ch4,30,introduction to information retrieval sec postings compression
IR_ch4,31,introduction to information retrieval sec postings compression the postings file is much larger than the dictionary factor of at least often over times larger key desideratum store each posting compactly a posting for our purposes is a docid for reuters documents we would use bits per docid when using byte integers alternatively we can use log bits per docid our goal use far fewer than bits per docid
IR_ch4,32,introduction to information retrieval sec postings two conflicting forces a term like arachnocentric occurs in maybe one doc out of a million we would like to store this posting using log m bits a term like the occurs in virtually every doc so bits posting mb is too expensive prefer bitmap vector in this case k
IR_ch4,33,introduction to information retrieval sec gap encoding of postings file entries we store the list of docs containing a term in increasing order of docid computer consequence it suffices to store gaps hope most gaps can be encoded stored with far fewer than bits especially for common words
IR_ch4,34,introduction to information retrieval sec three postings entries
IR_ch4,35,introduction to information retrieval sec variable length encoding aim for arachnocentric we will use bits gap entry for the we will use bit gap entry if the average gap for a term is g we want to use log g bits gap entry key challenge encode every integer gap with about as few bits as needed for that integer this requires a variable length encoding variable length codes achieve this by using short codes for small numbers
IR_ch4,36,introduction to information retrieval unary code represent n as n s with a final unary code for is unary code for is unary code for is this doesn t look promising but optimal if p n n we can use it as part of our solution
IR_ch4,37,introduction to information retrieval sec gamma codes we can compress better with bit level codes the gamma code is the best known of these represent a gap g as a pair length and offset offset is g in binary with the leading bit cut off for example length is the length of offset for offset this is we encode length with unary code gamma code of is the concatenation of length and offset
IR_ch4,38,introduction to information retrieval sec gamma code examples number length offset code none
IR_ch4,39,introduction to information retrieval reminder bitwise operations for compression you need to use bitwise operators python and most everything else bitwise and bitwise or bitwise xor ones complement left shift bits right shift lacks zero fill right shift recipes extract bits a x f if take high order bit add x f combine bit numbers a b c lookup tables rather than decoding can be faster yet still small
IR_ch4,40,introduction to information retrieval sec gamma code properties g is encoded using log g bits length of offset is log g bits length of length is log g bits all gamma codes have an odd number of bits almost within a factor of of best possible log g gamma code is uniquely prefix decodable like vb gamma code can be used for any distribution optimal for p n n gamma code is parameter free
IR_ch4,41,introduction to information retrieval sec gamma seldom used in practice machines have word boundaries bits operations that cross word boundaries are slower compressing and manipulating at the granularity of bits can be too slow all modern practice is to use byte or word aligned codes variable byte encoding is a faster conceptually simpler compression scheme with decent compression
IR_ch4,42,introduction to information retrieval sec variable byte vb codes for a gap value g we want to use close to the fewest bytes needed to hold log g bits begin with one byte to store g and dedicate bit in it to be a continuation bit c if g binary encode it in the available bits and set c else encode g s lower order bits and then use additional bytes to encode the higher order bits using the same algorithm at the end set the continuation bit of the last byte to c and for the other bytes c
IR_ch4,43,introduction to information retrieval sec example docids gaps vb code postings stored as the byte concatenation key property vb encoded postings are uniquely prefix decodable for a small gap vb uses a whole byte
IR_ch4,44,introduction to information retrieval sec rcv compression data structure size in mb dictionary fixed width dictionary term pointers into string with blocking k with blocking front coding collection text xml markup etc collection text term doc incidence matrix postings uncompressed bit words postings uncompressed bits postings variable byte encoded postings encoded
IR_ch4,45,introduction to information retrieval sec other variable unit codes variable byte codes are used by many real systems good low tech blend of variable length coding and sensitivity to computer memory alignment matches byte alignment wastes space if you have many small gaps as gap encoding often makes more modern work mainly uses the ideas be word aligned or bits even faster encode several gaps at the same time often assume a maximum gap size perhaps with an escape
IR_ch4,46,introduction to information retrieval group variable integer code used by google around turn of millennium jeff dean keynote at wsdm and presentations at cs encodes integers in blocks of size bytes first byte four bit binary length fields l l l l l j then l l l l bytes between hold numbers each number can use bits max gap length billion it was suggested that this was about twice as fast as vb encoding decoding gaps is much simpler no bit masking first byte can be decoded with lookup table or switch
IR_ch4,47,introduction to information retrieval simple anh moffat a word aligned multiple number encoding scheme how can we store several numbers in bits with a format selector bit numbers bit numbers bit numbers bit numbers selectors total ways
IR_ch4,48,introduction to information retrieval simple encoding scheme anh moffat encoding block bytes bits most significant nibble bits describe the layout of the other bits as follows layout n numbers of b bits each a single bit number bits n b two bit numbers three bit numbers and one spare bit four bit numbers five bit numbers and three spare bits seven bit numbers nine bit numbers and one spare bit fourteen two bit numbers twenty eight one bit numbers simple is a variant with additional uneven configurations efficiently decoded with hand coded decoder using bit masks extended simple family idea applies to bit words etc
IR_ch4,49,introduction to information retrieval sec index compression summary we can now create an index for highly efficient boolean retrieval that is very space efficient only of the total size of the collection only of the total size of the text in the collection we ve ignored positional information hence space savings are less for indexes used in practice but techniques substantially the same
IR_ch4,50,introduction to information retrieval ch resources for today s lecture iir mg f scholer h e williams and j zobel compression of inverted indexes for fast query evaluation proc acm sigir variable byte codes v n anh and a moffat inverted index compression using word aligned binary codes information retrieval word aligned codes
IR_ch5,1,introduction to information retrieval introduction to information retrieval cs information retrieval and web search christopher manning and pandu nayak wildcard queries and spelling correction
IR_ch5,2,introduction to information retrieval wild card queries
IR_ch5,3,introduction to information retrieval sec wild card queries mon find all docs containing any word beginning with mon easy with binary tree or b tree dictionary retrieve all words in range mon w moo mon find words ending in mon harder maintain an additional b tree for terms backwards can retrieve all words in range nom w non from this how can we enumerate all terms meeting the wild card query pro cent
IR_ch5,4,introduction to information retrieval sec query processing at this point we have an enumeration of all terms in the dictionary that match the wild card query we still have to look up the postings for each enumerated term e g consider the query se ate and fil er this may result in the execution of many boolean and queries
IR_ch5,5,introduction to information retrieval sec b trees handle s at the end of a query term how can we handle s in the middle of query term co tion we could look up co and tion in a b tree and intersect the two term sets expensive the solution transform wild card queries so that the s occur at the end this gives rise to the permuterm index
IR_ch5,6,introduction to information retrieval sec permuterm index add a to the end of each term rotate the resulting term and index them in a b tree for term hello index under hello ello h llo he lo hel o hell hello where is a special symbol hello ello h llo he empirically dictionary hello quadruples in size lo hel o hell hello
IR_ch5,7,introduction to information retrieval sec permuterm query processing add rotate to end lookup in permuterm index queries x lookup on x hello for hello x lookup on x hel for hel x lookup on x llo for llo x lookup on x ell for ell x y lookup on y x lo h for h lo x y z treat as a search for x z and post filter for h a o search for h o by looking up o h and post filter hello and retain halo
IR_ch5,8,introduction to information retrieval sec bigram k gram indexes enumerate all k grams sequence of k chars occurring in any term e g from text april is the cruelest month we get the grams bigrams a ap pr ri il l i is s t th he e c cr ru ue el le es st t m mo on nt h is a special word boundary symbol maintain a second inverted index from bigrams to dictionary terms that match each bigram
IR_ch5,9,introduction to information retrieval sec bigram index example the k gram index finds terms based on a query consisting of k grams here k m mace madden mo among amortize on along among
IR_ch5,10,introduction to information retrieval sec processing wild cards query mon can now be run as m and mo and on gets terms that match and version of our wildcard query but we d enumerate moon must post filter these terms against query surviving enumerated terms are then looked up in the term document inverted index fast space efficient compared to permuterm
IR_ch5,11,introduction to information retrieval sec processing wild card queries as before we must execute a boolean query for each enumerated filtered term wild cards can result in expensive query execution very large disjunctions pyth and prog if you encourage laziness people will respond search type your search terms use if you need to e g alex will match alexander
IR_ch5,12,introduction to information retrieval spelling correction
IR_ch5,13,introduction to information retrieval applications for spelling correction word processing phones web search
IR_ch5,14,introduction to information retrieval rates of spelling errors depending on the application error rates web queries wang et al retyping no backspace whitelaw et al english german words corrected retyping on phone sized organizer words uncorrected on organizer soukoreff mackenzie retyping kane and wobbrock gruden et al
IR_ch5,15,introduction to information retrieval spelling tasks spelling error detection spelling error correction autocorrect hte the suggest a correction suggestion lists
IR_ch5,16,introduction to information retrieval types of spelling errors non word errors graffe giraffe real word errors typographical errors three there cognitive errors homophones piece peace too two your you re non word correction was historically mainly context insensitive real word correction almost needs to be context sensitive
IR_ch5,17,introduction to information retrieval non word spelling errors non word spelling error detection any word not in a dictionary is an error the larger the dictionary the better up to a point the web is full of mis spellings so the web isn t necessarily a great dictionary non word spelling error correction generate candidates real words that are similar to error choose the one which is best shortest weighted edit distance highest noisy channel probability
IR_ch5,18,introduction to information retrieval real word non word spelling errors for each word w generate candidate set find candidate words with similar pronunciations find candidate words with similar spellings include w in candidate set choose best candidate noisy channel view of spell errors context sensitive so have to consider whether the surrounding words make sense flying form heathrow to lax flying from heathrow to lax
IR_ch5,19,introduction to information retrieval terminology we just discussed character bigrams and k grams st pr an we can also have word bigrams and n grams palo alto flying from road repairs
IR_ch5,20,introduction to information retrieval the noisy channel model of spelling independent word spelling correction
IR_ch5,21,introduction to information retrieval noisy channel intuition
IR_ch5,22,introduction to information retrieval noisy channel bayes rule we see an observation x of a misspelled word find the correct word ŵ w ˆ a a r r gw gw mî mî v v a a x x p p w x p w x x p w bayes argmax p x w p w wîv prior noisy channel model
IR_ch5,23,introduction to information retrieval history noisy channel for spelling proposed around ibm mays eric fred j damerau and robert l mercer context based spelling correction information processing and management at t bell labs kernighan mark d kenneth w church and william a gale a spelling correction program based on a noisy channel model proceedings of coling
IR_ch5,24,introduction to information retrieval non word spelling error example acress
IR_ch5,25,introduction to information retrieval candidate generation words with similar spelling small edit distance to error words with similar pronunciation small distance of pronunciation to error
IR_ch5,26,introduction to information retrieval candidate testing damerau levenshtein edit distance minimal edit distance between two strings where edits are insertion deletion substitution transposition of two adjacent letters see iir sec for edit distance
IR_ch5,27,introduction to information retrieval words within of acress error candidate correct error type correction letter letter acress actress t deletion acress cress a insertion acress caress ca ac transposition acress access c r substitution acress across o e substitution acress acres s insertion
IR_ch5,28,introduction to information retrieval candidate generation of errors are within edit distance almost all errors within edit distance also allow insertion of space or hyphen thisidea this idea inlaw in law can also allow merging words data base database for short texts like a query can just regard whole string as one item from which to produce edits
IR_ch5,29,introduction to information retrieval how do you generate the candidates run through dictionary check edit distance with each word generate all words within edit distance k e g k or and then intersect them with dictionary use a character k gram index and find dictionary words that share most k grams with word e g by jaccard coefficient see iir sec compute them fast with a levenshtein finite state transducer have a precomputed map of words to possible corrections
IR_ch5,30,introduction to information retrieval a paradigm we want the best spell corrections instead of finding the very best we find a subset of pretty good corrections say edit distance at most find the best amongst them these may not be the actual best this is a recurring paradigm in ir including finding the best docs for a query best answers best ads find a good candidate set find the top k amongst them and return them as the best
IR_ch5,31,introduction to information retrieval let s say we ve generated candidates now back to bayes rule we see an observation x of a misspelled word find the correct word ŵ w ˆ a a r r gw gw mî mî v v a a x x p p w x p x w x p w argmax p x w p w what s p w wîv
IR_ch5,32,introduction to information retrieval language model take a big supply of words your document collection with t tokens let c w occurrences of w in other applications you can take the supply to be typed queries suitably filtered when a static dictionary is inadequate p w c t w
IR_ch5,33,introduction to information retrieval unigram prior probability counts from words in corpus of contemporary english coca word frequency of p w word actress cress caress access across acres
IR_ch5,34,introduction to information retrieval channel model probability error model probability edit probability kernighan church gale misspelled word x x x x x m correct word w w w w w n p x w probability of the edit deletion insertion substitution transposition
IR_ch5,35,introduction to information retrieval computing error probability confusion matrix del x y count xy typed as x ins x y count x typed as xy sub x y count y typed as x trans x y count xy typed as yx insertion and deletion conditioned on previous character
IR_ch5,36,introduction to information retrieval confusion matrix for substitution
IR_ch5,37,introduction to information retrieval nearby keys
IR_ch5,38,introduction to information retrieval generating the confusion matrix peter norvig s list of errors peter norvig s list of counts of single edit errors all peter norvig s ngrams data links http norvig com ngrams
IR_ch5,39,introduction to information retrieval channel model kernighan church gale
IR_ch5,40,introduction to information retrieval smoothing probabilities add smoothing but if we use the confusion matrix example unseen errors are impossible they ll make the overall probability that seems too harsh e g in kernighan s chart q a and a q are both even though they re adjacent on the keyboard a simple solution is to add to all counts and then if there is a a character alphabet to normalize appropriately sub x w if substitution p x w count w a
IR_ch5,41,introduction to information retrieval channel model for acress candidate correct error x w p x w correction letter letter actress t c ct cress a a caress ca ac ac ca access c r r c across o e e o acres s es e acres s ss s
IR_ch5,42,introduction to information retrieval candidate correct error x w p x w p w corrnectoionisy lcethteranlnetteerl probability for acress p x w p w actress t c ct cress a a caress ca ac ac c a access c r r c across o e e o acres s es e acres s ss s
IR_ch5,43,introduction to information retrieval candidate correct error x w p x w p w noisy channel probability for acress correction letter letter p x w p w actress t c c t cress a a caress ca ac ac ca access c r r c across o e e o acres s es e acres s ss s
IR_ch5,44,introduction to information retrieval evaluation some spelling error test sets wikipedia s list of common english misspelling aspell filtered version of that list birkbeck spelling error corpus peter norvig s list of errors includes wikipedia and birkbeck for training or testing
IR_ch5,45,introduction to information retrieval context sensitive spelling correction spelling correction with the noisy channel
IR_ch5,46,introduction to information retrieval real word spelling errors leaving in about fifteen minuets to go to her house the design an construction of the system can they lave him my messages the study was conducted mainly be john black of spelling errors are real words kukich
IR_ch5,47,introduction to information retrieval context sensitive spelling error fixing for each word in sentence phrase query generate candidate set the word itself all single letter edits that are english words words that are homophones all of this can be pre computed choose best candidates noisy channel model
IR_ch5,48,introduction to information retrieval noisy channel for real word spell correction given a sentence x x x x n generate a set of candidates for each word x i candidate x x w w w candidate x x w w w candidate x x w w w n n n n n choose the sequence w that maximizes p w x x n wˆ argmax p w x wîv argmax p x w p w wîv
IR_ch5,49,introduction to information retrieval incorporating context words context sensitive spelling correction determining whether actress or across is appropriate will require looking at the context of use we can do this with a better language model you learned can learn a lot about language models in cs or cs n here we present just enough to be dangerous do the assignment a bigram language model conditions the probability of a word on just the previous word p w w p w p w w p w w n n n
IR_ch5,50,introduction to information retrieval incorporating context words for unigram counts p w is always non zero if our dictionary is derived from the document collection this won t be true of p w w we need to smooth k k we could use add smoothing on this conditional distribution but here s a better way interpolate a unigram and a bigram p w w λp w λ p w w li k k uni k bi k k p w w c w w c w bi k k k k k
IR_ch5,51,introduction to information retrieval all the important fine points note that we have several probability distributions for words keep them straight you might want need to work with log probabilities log p w w log p w log p w w log p w w n n n otherwise be very careful about floating point underflow our query may be words anywhere in a document we ll start the bigram estimate of a sequence with a unigram estimate often people instead condition on a start of sequence symbol but not good here because of this the unigram and bigram counts have different totals not a problem
IR_ch5,52,introduction to information retrieval using a bigram language model a stellar and versatile acress whose combination of sass and glamour counts from the corpus of contemporary american english with add smoothing p actress versatile p whose actress p across versatile p whose across p versatile actress whose x p versatile across whose x
IR_ch5,53,introduction to information retrieval using a bigram language model a stellar and versatile acress whose combination of sass and glamour counts from the corpus of contemporary american english with add smoothing p actress versatile p whose actress p across versatile p whose across p versatile actress whose x p versatile across whose x
IR_ch5,54,introduction to information retrieval noisy channel for real word spell correction two of thew to threw tao off thaw too on the two of thaw
IR_ch5,55,introduction to information retrieval noisy channel for real word spell correction two of thew to threw tao off thaw too on the two of thaw
IR_ch5,56,introduction to information retrieval simplification one error per sentence out of all possible sentences with one word replaced w w w w two off thew w w w w two of the w w w w too of thew choose the sequence w that maximizes p w
IR_ch5,57,introduction to information retrieval where to get the probabilities language model unigram bigram etc channel model same as for non word spelling correction plus need probability for no error p w w
IR_ch5,58,introduction to information retrieval probability of no error what is the channel probability for a correctly typed word p the the if you have a big corpus you can estimate this percent correct but this value depends strongly on the application error in words error in words error in words
IR_ch5,59,introduction to information retrieval peter norvig s thew example x w x w p x w p w p x w p w thew the ew e thew thew thew thaw e a thew threw h hr ew w thew thwe e
IR_ch5,60,introduction to information retrieval state of the art noisy channel we never just multiply the prior and the error model independence assumptions probabilities not commensurate instead weight them learn λ from a development test set w ˆ a r gw mî v a x p x w p w l
IR_ch5,61,introduction to information retrieval improvements to channel model allow richer edits brill and moore ent ant ph f le al incorporate pronunciation into channel toutanova and moore incorporate device into channel not all android phones need have the same error model but spell correction may be done at the system level
IR_ch6,1,introduction to information retrieval introduction to information retrieval cs information retrieval and web search pandu nayak and prabhakar raghavan lecture scoring term weighting and the vector space model
IR_ch6,2,introduction to information retrieval recap of lecture collection and vocabulary statistics heaps and zipf s laws dictionary compression for boolean indexes dictionary string blocks front coding postings compression gap encoding prefix unique codes variable byte and gamma codes collection text xml markup etc mb collection text term doc incidence matrix postings uncompressed bit words postings uncompressed bits postings variable byte encoded postings encoded
IR_ch6,3,introduction to information retrieval this lecture iir sections ranked retrieval scoring documents term frequency collection statistics weighting schemes vector space scoring
IR_ch6,4,introduction to information retrieval ch ranked retrieval thus far our queries have all been boolean documents either match or don t good for expert users with precise understanding of their needs and the collection also good for applications applications can easily consume s of results not good for the majority of users most users incapable of writing boolean queries or they are but they think it s too much work most users don t want to wade through s of results this is particularly true of web search
IR_ch6,5,introduction to information retrieval ch problem with boolean search feast or famine boolean queries often result in either too few or too many s results query standard user dlink hits query standard user dlink no card found hits it takes a lot of skill to come up with a query that produces a manageable number of hits and gives too few or gives too many
IR_ch6,6,introduction to information retrieval ranked retrieval models rather than a set of documents satisfying a query expression in ranked retrieval the system returns an ordering over the top documents in the collection for a query free text queries rather than a query language of operators and expressions the user s query is just one or more words in a human language in principle there are two separate choices here but in practice ranked retrieval has normally been associated with free text queries and vice versa
IR_ch6,7,introduction to information retrieval ch feast or famine not a problem in ranked retrieval when a system produces a ranked result set large result sets are not an issue indeed the size of the result set is not an issue we just show the top k results we don t overwhelm the user premise the ranking algorithm works
IR_ch6,8,introduction to information retrieval ch scoring as the basis of ranked retrieval we wish to return in order the documents most likely to be useful to the searcher how can we rank order the documents in the collection with respect to a query assign a score say in to each document this score measures how well document and query match
IR_ch6,9,introduction to information retrieval ch query document matching scores we need a way of assigning a score to a query document pair let s start with a one term query if the query term does not occur in the document score should be the more frequent the query term in the document the higher the score should be we will look at a number of alternatives for this
IR_ch6,10,introduction to information retrieval ch take jaccard coefficient recall from lecture a commonly used measure of overlap of two sets a and b jaccard a b a b a b jaccard a a jaccard a b if a b a and b don t have to be the same size always assigns a number between and
IR_ch6,11,introduction to information retrieval ch jaccard coefficient scoring example what is the query document match score that the jaccard coefficient computes for each of the two documents below query ides of march document caesar died in march document the long march
IR_ch6,12,introduction to information retrieval issues with jaccard for scoring it doesn t consider term frequency how many times a term occurs in a document rare terms in a collection are more informative than frequent terms jaccard doesn t consider this information we need a more sophisticated way of normalizing for length later in this lecture we ll use instead of a b a b jaccard for length normalization a b a b ch
IR_ch6,13,introduction to information retrieval sec recall lecture binary term document incidence matrix antony and cleopatra julius caesar the tempest hamlet othello macbeth antony brutus caesar calpurnia cleopatra mercy worser each document is represented by a binary vector v
IR_ch6,14,introduction to information retrieval sec term document count matrices consider the number of occurrences of a term in a document each document is a count vector in ℕv a column below antony and cleopatra julius caesar the tempest hamlet othello macbeth antony brutus caesar calpurnia cleopatra mercy worser
IR_ch6,15,introduction to information retrieval bag of words model vector representation doesn t consider the ordering of words in a document john is quicker than mary and mary is quicker than john have the same vectors this is called the bag of words model in a sense this is a step back the positional index was able to distinguish these two documents we will look at recovering positional information later in this course for now bag of words model
IR_ch6,16,introduction to information retrieval term frequency tf the term frequency tf of term t in document d is t d defined as the number of times that t occurs in d we want to use tf when computing query document match scores but how raw term frequency is not what we want a document with occurrences of the term is more relevant than a document with occurrence of the term but not times more relevant relevance does not increase proportionally with term frequency nb frequency count in ir
IR_ch6,17,introduction to information retrieval log frequency weighting the log frequency weight of term t in d is etc score for a document query pair sum over terms t in both q and d score the score is if none of the query terms is present in the document w t d t q l o d g t f t l d o g t f t i o d f t t h f e t d r w i s e sec
IR_ch6,18,introduction to information retrieval sec document frequency rare terms are more informative than frequent terms recall stop words consider a term in the query that is rare in the collection e g arachnocentric a document containing this term is very likely to be relevant to the query arachnocentric we want a high weight for rare terms like arachnocentric
IR_ch6,19,introduction to information retrieval sec document frequency continued frequent terms are less informative than rare terms consider a query term that is frequent in the collection e g high increase line a document containing such a term is more likely to be relevant than a document that doesn t but it s not a sure indicator of relevance for frequent terms we want high positive weights for words like high increase and line but lower weights than for rare terms we will use document frequency df to capture this
IR_ch6,20,introduction to information retrieval idf weight df is the document frequency of t the number of t documents that contain t df is an inverse measure of the informativeness of t t df n t we define the idf inverse document frequency of t by we use log n df instead of n df to dampen the effect t t of idf i d f t l o g n d f t sec will turn out the base of the log is immaterial
IR_ch6,21,introduction to information retrieval sec idf example suppose n million term df idf t t calpurnia animal sunday fly under the idf log n df t t there is one idf value for each term t in a collection
IR_ch6,22,introduction to information retrieval effect of idf on ranking does idf have an effect on ranking for one term queries like iphone idf has no effect on ranking one term queries idf affects the ranking of documents for queries with at least two terms for the query capricious person idf weighting makes occurrences of capricious count for much more in the final document ranking than occurrences of person
IR_ch6,23,introduction to information retrieval sec collection vs document frequency the collection frequency of t is the number of occurrences of t in the collection counting multiple occurrences example word collection frequency document frequency insurance try which word is a better search term and should get a higher weight
IR_ch6,24,introduction to information retrieval sec tf idf weighting the tf idf weight of a term is the product of its tf weight and its idf weight w log tf log n df t d t t d best known weighting scheme in information retrieval note the in tf idf is a hyphen not a minus sign alternative names tf idf tf x idf increases with the number of occurrences within a document increases with the rarity of the term in the collection
IR_ch6,25,introduction to information retrieval score for a document given a query there are many variants how tf is computed with without logs whether the terms in the query are also weighted s c o r e q d t q d t f i d f t d sec
IR_ch6,26,introduction to information retrieval binary count weight matrix c c a b c a l m w n t o n y r u t u s a e s a r l p u r n i e o p a t r e r c y o r s e r a a a n t o n y a n d c l e o p a t r a j u l i u s c a e s a r t h e t e m p e s t h a m l e t o t h e l l o m a c b e t h sec each document is now represented by a real valued vector of tf idf weights r v
IR_ch6,27,introduction to information retrieval sec documents as vectors so we have a v dimensional vector space terms are axes of the space documents are points or vectors in this space very high dimensional tens of millions of dimensions when you apply this to a web search engine these are very sparse vectors most entries are zero
IR_ch6,28,introduction to information retrieval sec queries as vectors key idea do the same for queries represent them as vectors in the space key idea rank documents according to their proximity to the query in this space proximity similarity of vectors proximity inverse of distance recall we do this because we want to get away from the you re either in or out boolean model instead rank more relevant documents higher than less relevant documents
IR_ch6,29,introduction to information retrieval sec formalizing vector space proximity first cut distance between two points distance between the end points of the two vectors euclidean distance euclidean distance is a bad idea because euclidean distance is large for vectors of different lengths
IR_ch6,30,introduction to information retrieval sec why distance is a bad idea the euclidean distance between q and d is large even though the distribution of terms in the query q and the distribution of terms in the document d are very similar
IR_ch6,31,introduction to information retrieval sec use angle instead of distance thought experiment take a document d and append it to itself call this document d semantically d and d have the same content the euclidean distance between the two documents can be quite large the angle between the two documents is corresponding to maximal similarity key idea rank documents according to angle with query
IR_ch6,32,introduction to information retrieval sec from angles to cosines the following two notions are equivalent rank documents in decreasing order of the angle between query and document rank documents in increasing order of cosine query document cosine is a monotonically decreasing function for the interval o o
IR_ch6,33,introduction to information retrieval sec from angles to cosines but how and why should we be computing cosines
IR_ch6,34,introduction to information retrieval sec length normalization a vector can be length normalized by dividing each of its components by its length for this we use the l norm x x i i dividing a vector by its l norm makes it a unit length vector on surface of unit hypersphere effect on the two documents d and d d appended to itself from earlier slide they have identical vectors after length normalization long and short documents now have comparable weights
IR_ch6,35,introduction to information retrieval cosine query document c o s q d q q d d q q d d v i q v i i q i d i v i d i sec dot product unit vectors q is the tf idf weight of term i in the query i d is the tf idf weight of term i in the document i cos q d is the cosine similarity of q and d or equivalently the cosine of the angle between q and d
IR_ch6,36,introduction to information retrieval cosine for length normalized vectors for length normalized vectors cosine similarity is simply the dot product or scalar product v cos q d q d q d i i i for q d length normalized
IR_ch6,37,introduction to information retrieval cosine similarity illustrated
IR_ch6,38,introduction to information retrieval sec cosine similarity amongst documents how similar are the novels term sas pap wh sas sense and affection sensibility jealous pap pride and gossip wuthering prejudice and wh wuthering term frequencies counts heights note to simplify this example we don t do idf weighting
IR_ch6,39,introduction to information retrieval sec documents example contd log frequency weighting after length normalization term sas pap wh term sas pap wh affection affection jealous jealous gossip gossip wuthering wuthering cos sas pap cos sas wh cos pap wh why do we have cos sas pap cos sas wh
IR_ch6,40,introduction to information retrieval sec computing cosine scores
IR_ch6,41,introduction to information retrieval sec tf idf weighting has many variants columns headed n are acronyms for weight schemes why is the base of the log in idf immaterial
IR_ch6,42,introduction to information retrieval sec weighting may differ in queries vs documents many search engines allow for different weightings for queries vs documents smart notation denotes the combination in use in an engine with the notation ddd qqq using the acronyms from the previous table a very standard weighting scheme is lnc ltc document logarithmic tf l as first character no idf and cosine normalization a bad idea query logarithmic tf l in leftmost column idf t in second column no normalization
IR_ch6,43,introduction to information retrieval sec tf idf example lnc ltc document car insurance auto insurance query best car insurance term query document pro d tf tf wt df idf wt n liz tf raw tf wt wt n liz raw e e auto best car insurance exercise what is n the number of docs doc length score
IR_ch6,44,introduction to information retrieval summary vector space ranking represent the query as a weighted tf idf vector represent each document as a weighted tf idf vector compute the cosine similarity score for the query vector and each document vector rank documents with respect to the query by score return the top k e g k to the user
IR_ch6,45,introduction to information retrieval ch resources for today s lecture iir http www miislita com information retrieval tutorial cosine similarity tutorial html term weighting and cosine similarity tutorial for seo folk
IR_ch7,1,introduction to information retrieval introduction to information retrieval probabilistic information retrieval christopher manning and pandu nayak
IR_ch7,2,introduction to information retrieval ch from boolean to ranked retrieval why ranked retrieval introduction to the classical probabilistic retrieval model and the probability ranking principle the binary independence model bim relevance feedback briefly the vector space model vsm quick cameo bm model ranking with features bm f if time allows
IR_ch7,3,introduction to information retrieval ch ranked retrieval thus far our queries have all been boolean documents either match or don t can be good for expert users with precise understanding of their needs and the collection can also be good for applications applications can easily consume s of results not good for the majority of users most users incapable of writing boolean queries or they are but they think it s too much work most users don t want to wade through s of results this is particularly true of web search
IR_ch7,4,introduction to information retrieval ch problem with boolean search feast or famine boolean queries often result in either too few or too many s results query standard user dlink hits query standard user dlink no card found hits it takes a lot of skill to come up with a query that produces a manageable number of hits and gives too few or gives too many suggested solution rank documents by goodness a sort of clever soft and
IR_ch7,5,introduction to information retrieval why probabilities in ir understanding user query information need of user need is representation uncertain how to match uncertain guess of document whether document documents representation has relevant content in traditional ir systems matching between each document and query is attempted in a semantically imprecise space of index terms probabilities provide a principled foundation for uncertain reasoning can we use probabilities to quantify our search uncertainties
IR_ch7,6,introduction to information retrieval probabilistic ir topics classical probabilistic retrieval model probability ranking principle etc binary independence model naïve bayes text cat okapi bm bayesian networks for text retrieval language model approach to ir iir ch an important development in s ir probabilistic methods are one of the oldest but also one of the currently hot topics in ir traditionally neat ideas but didn t win on performance it seems to be different now
IR_ch7,7,introduction to information retrieval who are these people karen spärck jones karen spärck jones stephen robertson keith van rijsbergen
IR_ch7,8,introduction to information retrieval the document ranking problem we have a collection of documents user issues a query a list of documents needs to be returned ranking method is the core of modern ir systems in what order do we present documents to the user we want the best document to be first second best second etc idea rank by probability of relevance of the document w r t information need p r document query i
IR_ch7,9,introduction to information retrieval the probability ranking principle prp if a reference retrieval system s response to each request is a ranking of the documents in the collection in order of decreasing probability of relevance to the user who submitted the request where the probabilities are estimated as accurately as possible on the basis of whatever data have been made available to the system for this purpose the overall effectiveness of the system to its user will be the best that is obtainable on the basis of those data s s s robertson w s cooper m e maron van rijsbergen manning schütze
IR_ch7,10,introduction to information retrieval for events a and b bayes rule prior odds p p a a b b p p a b ç p a b b p a p a å b x p p a b b a p a b p p b x a a p x p a recall a few probability basics posterior p a p a o a p a p a
IR_ch7,11,introduction to information retrieval the probability ranking principle prp let x represent a document in the collection let r represent relevance of a document w r t given fixed query and let r represent relevant and r not relevant p p r r x x p p x x r r p p x p x p r r need to find p r x probability that a document x is relevant p r p r prior probability of retrieving a relevant or non relevant document at random p x r p x r probability that if a relevant not relevant document is retrieved it is x p r x p r x
IR_ch7,12,introduction to information retrieval probabilistic retrieval strategy first estimate how each term contributes to relevance how do other things like term frequency and document length influence your judgments about document relevance not at all in bim a more nuanced answer is given by bm combine to find document relevance probability order documents by decreasing probability theorem using the prp is optimal in that it minimizes the loss bayes risk under loss provable if all probabilities correct etc e g ripley
IR_ch7,13,introduction to information retrieval binary independence model traditionally used in conjunction with prp binary boolean documents are represented as binary incidence vectors of terms cf iir chapter iff term i is present in document x independence terms occur in documents independently different documents can be modeled as the same vector x x i x x n
IR_ch7,14,introduction to information retrieval binary independence model queries binary term incidence vectors given query q for each document d need to compute p r q d replace with computing p r q x where x is binary term incidence vector representing d interested only in ranking will use odds and bayes rule p r q p x r q p r q x p x q o r q x p r q p x r q p r q x p x q
IR_ch7,15,introduction to information retrieval binary independence model using independence assumption n p x r q õ o r q x o r q i p x r q i i p p x x r r q q õ i n p p x x i i r r q q p r q x p r q p x r q o r q x p r q x p r q p x r q constant for a needs estimation given query
IR_ch7,16,introduction to information retrieval binary independence model since x is either or i p x r q p x r q õ õ o r q x o r q i i p x r q p x r q x i x i i i let p i p x i r q r p x r q i i p r assume for all terms not occurring in the query q i i i o r q x o r q õ i n p p x x i i r r q q p p õ õ o r q x o r q i i r r x i x i i i q q i i
IR_ch7,17,introduction to information retrieval document relevant r not relevant r term present x p r i i i term absent x p r i i i
IR_ch7,18,introduction to information retrieval binary independence model all matching terms non matching query terms æ ö p r p p õ õ õ o r q x o r q i ç i i i r p r r è ø x i x i i x i i i i q q q i i i p r p õ õ o r q x o r q i i i r p r x q i i q i i i i all matching terms all query terms o r q x o r q x õ i q i p r i i õ xq i i p r i i
IR_ch7,19,introduction to information retrieval binary independence model constant for each query only quantity to be estimated for rankings o r q x o r q x i q i r p i i p r i i q i p r i i retrieval status value p r p r rsv log i i log i i r p r p x q i i x q i i i i i i
IR_ch7,20,introduction to information retrieval binary independence model robertson spärck jones all boils down to computing rsv r r s s v v l x o i g q i x i c i q i r p i c i i l p o r i i g r p i i x i q i p r i i l o g r p i i p r i i the c are log odds ratios of contingency table a few slides back i they function as the term weights in this model so how do we compute c s from our data i
IR_ch7,21,introduction to information retrieval graphical model for bim bernoulli nb r binary x i variables i î q x i t f i ¹
IR_ch7,22,introduction to information retrieval binary independence model estimating rsv coefficients in theory for each term i look at this table of document counts d x x t o i i o c t a u l m e n t s p i r e l e s s s v s s a s n t n o n n r n n n e l s s s e v a s n t t o n t n n a l n n s estimates for now r i n s assume no zero terms s s s c k n n s s log remember i n s n n s s smoothing c i l o g p r i i p r i i
IR_ch7,23,introduction to information retrieval estimation key challenge if non relevant documents are approximated by the whole collection then r prob of occurrence i in non relevant documents for query is n n and inverse document frequency idf spärck jones a key still important term weighting concept l o g r i r i l o g n n n s s s l o g n n n l o g n n i d f
IR_ch7,24,introduction to information retrieval sec collection vs document frequency collection frequency of t is the total number of occurrences of t in the collection incl multiples document frequency is number of docs t is in example word collection frequency document frequency insurance try which word is a better search term and should get a higher weight
IR_ch7,25,introduction to information retrieval estimation key challenge p probability of occurrence in relevant i documents cannot be approximated as easily p can be estimated in various ways i from relevant documents if you know some relevance weighting can be used in a feedback loop constant croft and harper combination match then just get idf weighting of terms with p i proportional to prob of occurrence in collection greiff sigir argues for df n i r s v x å i q i l o g n n i
IR_ch7,26,introduction to information retrieval probabilistic relevance feedback guess a preliminary probabilistic description of r documents use it to retrieve a set of documents interact with the user to refine the description learn some definite members with r and r re estimate p and r on the basis of these i i if i appears in v within set of documents v p v v i i i or can combine new information with original guess use bayesian prior v p p i i κ is i v prior weight repeat thus generating a succession of approximations to relevant documents
IR_ch7,27,introduction to information retrieval pseudo relevance feedback iteratively auto estimate p and r i i assume that p is constant over all x in query and r i i i as before p even odds for any given doc i determine guess of relevant document set v is fixed size set of highest ranked documents on this model we need to improve our guesses for p and r so i i use distribution of x in docs in v let v be set of i i documents containing x i p v v i i assume if not retrieved then not relevant r n v n v i i i go to until converges then return ranking
IR_ch7,28,introduction to information retrieval prp and bim it is possible to reasonably approximate probabilities but either require partial relevance information or need to make do with somewhat inferior term weights requires restrictive assumptions relevance of each document is independent of others really it s bad to keep on returning duplicates term independence terms not in query don t affect the outcome boolean representation of documents queries boolean notion of relevance some of these assumptions can be removed
IR_ch7,29,introduction to information retrieval removing term independence in general index terms aren t independent hong kong dependencies can be complex van rijsbergen proposed simple model of dependencies as a tree each term dependent on one other exactly friedman and goldszmidt s tree augmented naive bayes aaai in s estimation problems held back success of this model
IR_ch7,30,introduction to information retrieval term frequency and the vsm right in the first lecture we said that a page should rank higher if it mentions a word more perhaps modulated by things like page length why not in bim much of early ir was designed for titles or abstracts and not for modern full text search we now want a model with term frequency in it we ll mainly look at a probabilistic model bm first a quick summary of vector space model
IR_ch7,31,introduction to information retrieval summary vector space ranking ch represent the query as a weighted term frequency inverse document frequency tf idf vector represent each document as a weighted tf idf vector compute the cosine similarity score for the query vector and each document vector rank documents with respect to the query by score return the top k e g k to the user
IR_ch7,32,introduction to information retrieval
IR_ch7,33,introduction to information retrieval sec cosine similarity
IR_ch7,34,introduction to information retrieval sec tf idf weighting has many variants
IR_ch7,35,introduction to information retrieval bm
IR_ch7,36,introduction to information retrieval okapi bm robertson et al trec city u bm best match they had a bunch of tries developed in the context of the okapi system started to be increasingly adopted by other teams during the trec competitions it works well goal be sensitive to term frequency and document length while not adding too many parameters robertson and zaragoza spärck jones et al
IR_ch7,37,introduction to information retrieval generative model for documents words are drawn independently from the vocabulary using a multinomial distribution the draft is that each team is given a position in the draft basic given draft team the each nfl annual team of draft design nfl football is that football
IR_ch7,38,introduction to information retrieval generative model for documents distribution of term frequencies tf follows a binomial distribution approximated by a poisson the draft is that each team is given a position in the draft draft
IR_ch7,39,introduction to information retrieval poisson distribution the poisson distribution models the probability of k the number of events occurring in a fixed interval of time space with known average rate λ cf t independent of the last event examples number of cars arriving at a toll booth per minute number of typos on a page p k l k k e l
IR_ch7,40,introduction to information retrieval poisson distribution if t is large and p is small we can approximate a binomial distribution with a poisson where λ tp k l l p k e k mean variance λ tp example p t chance of occurrence is binomial poisson already close p e e p æçè ö ø
IR_ch7,41,introduction to information retrieval poisson model assume that term frequencies in a document tf i follow a poisson distribution fixed interval implies fixed document length think roughly constant sized document abstracts will fix later
IR_ch7,42,introduction to information retrieval poisson distributions
IR_ch7,43,introduction to information retrieval one poisson model flaw is a reasonable fit for general words is a poor fit for topic specific words get higher p k than predicted too often documents containing k occurrences of word λ freq word expected based conditions cathexis comic harter a probabilistic approach to automatic keyword indexing jasist
IR_ch7,44,introduction to information retrieval eliteness aboutness model term frequencies using eliteness what is eliteness hidden variable for each document term pair denoted as e for term i i represents aboutness a term is elite in a document if in some sense the document is about the concept denoted by the term eliteness is binary term occurrences depend only on eliteness but eliteness depends on relevance
IR_ch7,45,introduction to information retrieval elite terms text from the wikipedia page on the nfl draft showing elite terms the national football league draft is an annual event in which the national football league nfl teams select eligible college football players it serves as the league s most common source of player recruitment the basic design of the draft is that each team is given a position in the draft order in reverse order relative to its record
IR_ch7,46,introduction to information retrieval graphical model with eliteness r binary e i variables frequencies tf i not binary i î q
IR_ch7,47,introduction to information retrieval retrieval status value similar to the bim derivation we have where and using eliteness we have r s v e l i t e i î å q t f i c e i l i t e t f i p tf tf r p tf tf e elite p e elite r i i i i i i p tf tf e elite p e elite r i i i i c e i l i t e t f i l o g p p t t f f i i t f i r r p p t t f f i i t f i r r
IR_ch7,48,introduction to information retrieval poisson model the problems with the poisson model suggests fitting two poisson distributions in the poisson model the distribution is different depending on whether the term is elite or not where π is probability that document is elite for term but unfortunately we don t know π λ μ p t f i k i r
IR_ch7,49,introduction to information retrieval elite c tf let s get an idea graphing for i i different parameter values of the poisson
IR_ch7,50,introduction to information retrieval qualitative properties c elite i increases monotonically with tf i but asymptotically approaches a maximum value as not true for simple scaling of tf with the asymptotic limit being c e i l i t e t f i weight of bim c eliteness i feature t f i
IR_ch7,51,introduction to information retrieval approximating the saturation function estimating parameters for the poisson model is not easy so approximate it with a simple parametric curve that has the same qualitative properties tf k tf
IR_ch7,52,introduction to information retrieval saturation function for high values of k increments in tf continue to i contribute significantly to the score contributions tail off quickly for low values of k
IR_ch7,53,introduction to information retrieval early versions of bm version using the saturation function version bim simplification to idf k factor doesn t change ranking but makes term score when tf i similar to tf idf but term scores are bounded c b i m c b i v m t v f i t f i l o g c d b i n f i m i k t k k f i t f i t f t i f i
IR_ch7,54,introduction to information retrieval document length normalization longer documents are likely to have larger tf values i why might documents be longer verbosity suggests observed tf too high i larger scope suggests observed tf may be right i a real document collection probably has both effects so should apply some kind of partial normalization
IR_ch7,55,introduction to information retrieval document length normalization document length avdl average document length over collection length normalization component b full document length normalization b no document length normalization b æçè d l b å i î v t f b i a d v l d l ö ø b
IR_ch7,56,introduction to information retrieval document length normalization
IR_ch7,57,introduction to information retrieval okapi bm normalize tf using document length bm ranking function t f i t b f i n k tf c bm tf log i i i df k tf i i l o g d n f i k b k b a t f d v i l d l t f i å rsv bm cbm tf i i iîq
IR_ch7,58,introduction to information retrieval okapi bm k controls term frequency scaling k is binary model k large is raw term frequency b controls document length normalization b is no length normalization b is relative frequency fully scale by document length typically k is set around and b around iir sec discusses incorporating query term weighting and pseudo relevance feedback r s v b m å i î q l o g d n f i k b k b a t f d v i l d l t f i
IR_ch7,59,introduction to information retrieval why is bm better than vsm tf idf suppose your query is machine learning suppose you have documents with term counts doc learning machine doc learning machine tf idf log tf log n df doc doc bm k doc doc
IR_ch7,60,introduction to information retrieval ranking with features textual features zones title author abstract body anchors proximity non textual features file type file age page rank
IR_ch7,61,introduction to information retrieval ranking with zones straightforward idea apply your favorite ranking function bm to each zone separately combine zone scores using a weighted linear combination but that seems to imply that the eliteness properties of different zones are different and independent of each other which seems unreasonable
IR_ch7,62,introduction to information retrieval ranking with zones alternate idea assume eliteness is a term document property shared across zones but the relationship between eliteness and term frequencies are zone dependent e g denser use of elite topic words in title consequence first combine evidence across zones for each term then combine evidence across terms
IR_ch7,63,introduction to information retrieval bm f with zones calculate a weighted variant of total term frequency and a weighted variant of document length z z å å average dl tf v tf dl v len avdl i z zi z z across all z z documents where v is zone weight z tf is term frequency in zone z zi len is length of zone z z z is the number of zones
IR_ch7,64,introduction to information retrieval simple bm f with zones simple interpretation zone z is replicated v times z but we may want zone specific parameters k b idf r s v s i m p l e b m f å i î q l o g d n f i k b k b a t d v f i l d l t f i
IR_ch7,65,introduction to information retrieval bm f empirically zone specific length normalization i e zone specific b has been found to be useful z tf å tf v zi i z b z z b z æçè b z b z a l v e l n e z n z ö ø b z n k tf rsv bm f å log i df k tf iîq i i see robertson and zaragoza
IR_ch7,66,introduction to information retrieval ranking with non textual features assumptions usual independence assumption independent of each other and of the textual features allows us to factor out in bim style derivation relevance information is query independent usually true for features like page rank age type allows us to keep all non textual features in the bim style derivation where we drop non query terms p p f f j j f f j j r r
IR_ch7,67,introduction to information retrieval ranking with non textual features f å å rsv c tf lv f i i j j j where iîq j p f f r j j v f log j j p f f r j j and l is an artificially added free parameter to account for rescalings in the approximations care must be taken in selecting v depending on f j j e g explains why works well j f log l f j j j l f j j l j e x p f j l j bm rsv log pagerank
IR_ch7,68,introduction to information retrieval resources s e robertson and k spärck jones relevance weighting of search terms journal of the american society for information sciences c j van rijsbergen information retrieval nd ed london butterworths chapter http www dcs gla ac uk keith preface html k spärck jones s walker and s e robertson a probabilistic model of information retrieval development and comparative experiments part information processing and management s e robertson and h zaragoza the probabilistic relevance framework bm and beyond foundations and trends in information retrieval
IR_ch8,1,introduction to information retrieval introduction to information retrieval evaluation chris manning and pandu nayak cs information retrieval and web search
IR_ch8,2,introduction to information retrieval situation thanks to your stellar performance in cs you quickly rise to vp of search at internet retail giant nozama com your boss brings in her nephew sergey who claims to have built a better search engine for nozama do you laugh derisively and send him to rival tramlaw labs counsel sergey to go to stanford and take cs try a few queries on his engine and say not bad
IR_ch8,3,introduction to information retrieval sec what could you ask sergey how fast does it index number of documents hour incremental indexing nozama adds k products day how fast does it search latency and cpu needs for nozama s million products does it recommend related products this is all good but it says nothing about the quality of sergey s search you want nozama s users to be happy with the search experience
IR_ch8,4,introduction to information retrieval how do you tell if users are happy search returns products relevant to users how do you assess this at scale search results get clicked a lot misleading titles summaries can cause users to click users buy after using the search engine or users spend a lot of after using the search engine repeat visitors buyers do users leave soon after searching do they come back within a week month
IR_ch8,5,introduction to information retrieval sec happiness elusive to measure most common proxy relevance of search results pioneered by cyril cleverdon in the cranfield experiments but how do you measure relevance
IR_ch8,6,introduction to information retrieval sec measuring relevance three elements a benchmark document collection a benchmark suite of queries an assessment of either relevant or nonrelevant for each query and each document
IR_ch8,7,introduction to information retrieval so you want to measure the quality of a new search algorithm benchmark documents nozama s products benchmark query suite more on this judgments of document relevance for each query relevance million nozama com products judgment sample queries
IR_ch8,8,introduction to information retrieval relevance judgments binary relevant vs non relevant in the simplest case more nuanced relevance levels also used what are some issues already million times k takes us into the range of a quarter trillion judgments if each judgment took a human seconds we d still need seconds or nearly million if you pay people per hour to assess k new products per day
IR_ch8,9,introduction to information retrieval crowd source relevance judgments present query document pairs to low cost labor on online crowd sourcing platforms hope that this is cheaper than hiring qualified assessors lots of literature on using crowd sourcing for such tasks you get fairly good signal but the variance in the resulting judgments is quite high
IR_ch8,10,introduction to information retrieval sec what else still need test queries must be germane to docs available must be representative of actual user needs random query terms from the documents are not a good idea sample from query logs if available classically non web low query rates not enough query logs experts hand craft user needs
IR_ch8,11,introduction to information retrieval sec early public test collections th c typical trec recent datasets s of million web pages gov clueweb
IR_ch8,12,introduction to information retrieval now we have the basics of a benchmark let s review some evaluation measures precision recall dcg
IR_ch8,13,introduction to information retrieval sec evaluating an ir system note user need is translated into a query relevance is assessed relative to the user need not the query e g information need my swimming pool bottom is becoming black and needs to be cleaned query pool cleaner assess whether the doc addresses the underlying need not whether it has these words
IR_ch8,14,introduction to information retrieval sec unranked retrieval evaluation precision and recall recap from iir video binary assessments precision fraction of retrieved docs that are relevant p relevant retrieved recall fraction of relevant docs that are retrieved p retrieved relevant relevant nonrelevant retrieved tp fp not retrieved fn tn precision p tp tp fp recall r tp tp fn
IR_ch8,15,introduction to information retrieval rank based measures binary relevance precision k p k mean average precision map mean reciprocal rank mrr multiple levels of relevance normalized discounted cumulative gain ndcg
IR_ch8,16,introduction to information retrieval precision k set a rank threshold k compute relevant in top k ignores documents ranked lower than k ex prec of prec of prec of in similar fashion we have recall k
IR_ch8,17,introduction to information retrieval sec a precision recall curve lots more detail on this in the canvas video n o i s i c e r p recall
IR_ch8,18,introduction to information retrieval mean average precision consider rank position of each relevant doc k k k r compute precision k for each k k k r average precision average of p k ex has avgprec of map is average precision across multiple queries rankings
IR_ch8,19,introduction to information retrieval average precision
IR_ch8,20,introduction to information retrieval map
IR_ch8,21,introduction to information retrieval mean average precision if a relevant document never gets retrieved we assume the precision corresponding to that relevant doc to be zero map is macro averaging each query counts equally now perhaps most commonly used measure in research papers good for web search map assumes user is interested in finding many relevant documents for each query map requires many relevance judgments in text collection
IR_ch8,22,introduction to information retrieval beyond binary relevance
IR_ch8,23,introduction to information retrieval fair fair good
IR_ch8,24,introduction to information retrieval discounted cumulative gain popular measure for evaluating web search and related tasks two assumptions highly relevant documents are more useful than marginally relevant documents the lower the ranked position of a relevant document the less useful it is for the user since it is less likely to be examined
IR_ch8,25,introduction to information retrieval discounted cumulative gain uses graded relevance as a measure of usefulness or gain from examining a document gain is accumulated starting at the top of the ranking and may be reduced or discounted at lower ranks typical discount is log rank with base the discount at rank is and at rank it is
IR_ch8,26,introduction to information retrieval summarize a ranking dcg what if relevance judgments are in a scale of r r cumulative gain cg at rank n let the ratings of the n documents be r r r n in ranked order cg r r r n discounted cumulative gain dcg at rank n dcg r r log r log r log n n we may use any base for the logarithm
IR_ch8,27,introduction to information retrieval discounted cumulative gain dcg is the total gain accumulated at a particular rank p alternative formulation used by some web search companies emphasis on retrieving highly relevant documents
IR_ch8,28,introduction to information retrieval dcg example ranked documents judged on relevance scale discounted gain dcg
IR_ch8,29,introduction to information retrieval ndcg for summarizing rankings normalized discounted cumulative gain ndcg at rank n normalize dcg at rank n by the dcg value at rank n of the ideal ranking the ideal ranking would first return the documents with the highest relevance level then the next highest relevance level etc normalization useful for contrasting queries with varying numbers of relevant results ndcg is now quite popular in evaluating web search
IR_ch8,30,introduction to information retrieval ndcg example ground truth ranking function ranking function i document document document r r r order i order i order i d d d d d d d d d d d d ndcg ndcg ndcg gt rf rf dcg gt log log log d d c c g g r r f f l o l o g g l o l o g g l o l o g g documents d d d d maxdcg dcg gt
IR_ch8,31,introduction to information retrieval what if the results are not in a list suppose there s only one relevant document scenarios known item search navigational queries looking for a fact search duration rank of the answer measures a user s effort
IR_ch8,32,introduction to information retrieval mean reciprocal rank consider rank position k of first relevant doc could be only clicked doc reciprocal rank score mrr is the mean rr across multiple queries k
IR_ch8,33,introduction to information retrieval human judgments are expensive inconsistent between raters over time decay in value as documents query mix evolves not always representative of real users rating vis à vis query don t know underlying need may not understand meaning of terms etc so what alternatives do we have
IR_ch8,34,introduction to information retrieval using user clicks
IR_ch8,35,introduction to information retrieval taken with slight adaptation from fan guo and chao liu s cikm tutorial statistical user behavior models for web search click log analysis search results for cikm in of clicks received
IR_ch8,36,introduction to information retrieval user behavior adapt ranking to user clicks of clicks received
IR_ch8,37,introduction to information retrieval what do clicks tell us tools needed for non trivial cases of clicks received strong position bias so absolute click rates unreliable
IR_ch8,38,introduction to information retrieval eye tracking user study
IR_ch8,39,introduction to information retrieval click position bias higher positions receive more user attention eye e g a t fixation and clicks than n e c lower positions r e p this is true even in the normal position extreme setting where the order of positions is e reversed g a t n e c r e p clicks are informative but biased reversed impression joachims
IR_ch8,40,introduction to information retrieval relative vs absolute ratings user s click sequence hard to conclude result result probably can conclude result result
IR_ch8,41,introduction to information retrieval evaluating pairwise relative ratings pairs of the form doca better than docb for a query doesn t mean that doca relevant to query now rather than assess a rank ordering wrt per doc relevance assessments assess in terms of conformance with historical pairwise preferences recorded from user clicks but don t learn and test on the same ranking algorithm i e if you learn historical clicks from nozama and compare sergey vs nozama on this history
IR_ch8,42,introduction to information retrieval comparing two rankings via clicks joachims query support vector machines ranking a ranking b kernel machines kernel machines svm light svms lucent svm demo intro to svms royal holl svm archives of svm svm software svm light svm tutorial svm software
IR_ch8,43,introduction to information retrieval interleave the two rankings kernel machines kernel machines svms this interleaving svm light starts with b intro to svms lucent svm demo archives of svm royal holl svm svm light
IR_ch8,44,introduction to information retrieval remove duplicate results kernel machines kernel machines svms svm light intro to svms lucent svm demo archives of svm royal holl svm svm light
IR_ch8,45,introduction to information retrieval count user clicks kernel machines a b kernel machines clicks svms ranking a svm light a ranking b intro to svms lucent svm demo a archives of svm royal holl svm svm light
IR_ch8,46,introduction to information retrieval interleaved ranking present interleaved ranking to users start randomly with ranking a or ranking b to even out presentation bias count clicks on results from a versus results from b better ranking will on average get more clicks
IR_ch8,47,introduction to information retrieval sec a b testing at web search engines purpose test a single innovation prerequisite you have a large search engine up and running have most users use old system divert a small proportion of traffic e g to an experiment to evaluate an innovation interleaved experiment full page experiment
IR_ch8,48,introduction to information retrieval facts entities what happens to clicks
IR_ch8,49,introduction to information retrieval recap benchmarks consist of document collection query set assessment methodology assessment methodology can use raters user clicks or a combination these get quantized into a goodness measure precision ndcg etc different engines algorithms compared on a benchmark together with a goodness measure
IR_ch8,50,introduction to information retrieval user behavior user behavior is an intriguing source of relevance data users make somewhat informed choices when they interact with search engines potentially a lot of data available in search logs but there are significant caveats user behavior data can be very noisy interpreting user behavior can be tricky spam can be a significant problem not all queries will have user behavior
IR_ch8,51,introduction to information retrieval incorporating user behavior into ranking algorithm incorporate user behavior features into a ranking function like bm f but requires an understanding of user behavior features so that appropriate v functions are used j incorporate user behavior features into learned ranking function either of these ways of incorporating user behavior signals improve ranking
